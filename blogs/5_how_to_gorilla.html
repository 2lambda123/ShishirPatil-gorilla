<!DOCTYPE html>
<html>
<head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NRZJLJCSH6"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-NRZJLJCSH6');
    </script>


    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Introduction to Gorilla LLM</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro">
    <link rel="stylesheet" href="assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="assets/css/styles.css">
    <link rel="stylesheet" href="assets/css/Team-Clean.css">
</head>

<body>
    <!-- Navigation Bar -->
    <div class="navbar" style="position: absolute; top: 0; right: 20px; padding: 10px; z-index: 100;">
        <a href="../index.html">Home </a>&nbsp;&nbsp;&nbsp;     
        <a href="../blog.html"> Blog</a>
    </div>
    <div class="highlight-clean-blog" style="padding-bottom: 10px;">
        <h1 class="text-center" style="padding-bottom: 10px;"> ü¶ç Gorilla: Large Language Model Connected with Massive APIs</h1>

        <div class="box-index">
            <h3>Blog 5: How to Use Gorilla</h3>
            <ul>
                <ul>
                        <li><a href="5_how_to_gorilla.html#gorilla-cli">Gorilla CLI: Gorilla in Your Command Line</a></li>
                        <li><a href="5_how_to_gorilla.html#gorilla-inference">Gorilla Inference</a></li>
                        <li><a href="5_how_to_gorilla.html#gorilla-openfunctions">Gorilla Open Functions: Executable API Calls</a></li>
                        <li><a href="5_how_to_gorilla.html#integrate-third-party">Integrating Gorilla with Third Party Libraries</a></li>
                        <li><a href="5_how_to_gorilla.html#api-zoo">Gorilla API Zoo Index</a></li>
                        <li><a href="5_how_to_gorilla.html#train-your-own-gorilla">Train Your Own Gorilla</a></li>
                        <li class="more-blogs">
                            <a href="javascript:void(0);" onclick="toggleMoreBlogs()">More Blogs <span class="caret">&#9654;</span></a>
                            <ul class="sub-menu">
                                <li><a href="1_gorilla_intro.html">Introduction to Gorilla LLM</a></li>
                                <li><a href="2_hallucination.html">How to Measure Hallucination?</a></li>
                                <li><a href="3_retreiver_aware_training.html">Retrieval Aware Training (RAT)</a></li>
                                <li><a href="4_open_functions.html">Gorilla OpenFunctions</a></li>
                                <!-- Add more blog entries as needed -->
                            </ul>
                        </li>  
                </ul>
                <!-- Add more entries as needed -->
            </ul>
        </div>

        <div class="blog-container">
            <div class="blog-post">
                <h2 class="blog-title">How to Use Gorilla </h2>
                <div class="author-date">
                    <p class="author"> <a href="https://www.linkedin.com/in/pranav--ramesh/"> Pranav Ramesh </a>  </p>
                    <p class="date">Feb 15, 2024</p>
                </div>
                <!-- Image here -->
                <div class="Introduction">
                    <p>
                        Gorilla is a rapidly growing open-source project that connects LLM‚Äôs to APIs. We currently have 6 different ways to utilize Gorilla.
                        <ol>
                            <li>Gorilla CLI
                            </li>
                            <li>Gorilla Inference
                            </li>
                            <li>Gorilla Open Functions
                            </li>
                            <li>Integrating Gorilla With Third Party Libraries
                            </li>
                            <li>Gorilla API Zoo Index
                            </li>
                            <li>Train Your Own Gorilla
                            </li>
                        </ol>
                        This blog post outlines these ways in a step-by-step manner and links Google Colab playground environments for you to quickly experiment with Gorilla!
                    </p>>
                </div>

                <div class="body">
                    <h4 id="gorilla-cli"><a href="https://github.com/gorilla-llm/gorilla-cli#usage">Gorilla CLI: Gorilla in Your Command Line üíª</a></h4>
                    <p>
                        Gorilla CLI simplifies command line interactions by generating candidate commands for your desired task. 
                        Instead of needing to recall verbose commands for various API‚Äôs, Gorilla CLI streamlines this process into a simple command: <code>gorilla ‚Äúyour query‚Äù</code>. 
                        Moreover, we prioritize confidentiality and full user control, only executing commands with explicit permission from the user and never collecting output data. 
                        <ol>
                            <li>Get started with pip install: 
                                <br><code>pip install gorilla-cli</code>>
                            </li>
                            <li> Using gorilla CLI to write to files:
                                <ul>
                                    <li>Command:
                                    <br><code>$ gorilla ‚Äúgenerate 100 random characters into a file called text.txt‚Äù</code>
                                    </li>
                                    <li>Output: Using arrow keys, toggle between commands and press <code>Enter</code>
                                    <br><code>¬ª cat /dev/urandom | env LC_ALL=C tr -dc 'a-zA-Z0-9' | head -c 100 > test.txt </code>
                                    <br><code>echo $(head /dev/urandom | LC_CTYPE=C tr -dc 'a-zA-Z0-9' | dd bs=100 count=1) > test.txt </code>
                                    <br><code>dd if=/dev/urandom bs=1 count=100 of=test.txt </code>
                                    </li>
                                </ul>
                            </li>
                            <li> Other Usage Examples:
                                <ul>
                                    <li>Command:
                                    <br><code>$ gorilla ‚Äúlist all my GCP instances‚Äù</code>
                                    </li>
                                    <li>Output: 
                                    <br><code>¬ª gcloud compute instances list --format="table(name,zone,status)"</code>
                                    <br><code>gcloud compute instances list --format table </code>
                                    <br><code>gcloud compute instances list --format="table(name, zone, machineType, status)" </code>
                                    </li>
                                </ul>
                            </li>
                        </ol>
                    </p>
                    <h4 id="gorilla-inference"><a href="https://github.com/ShishirPatil/gorilla/tree/main/inference">Gorilla Inference ü¶ç </a></h4>
                    <p>
                        <i>Note: If you would like to try out zero-shot Gorilla in a playground-like environment check out <a href="https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing">this Google Colab</a>. 
                        </i>
                        <br>
                        <br>Running inference with gorilla is now supported on both hosted endpoints and locally! Inference with Gorilla can be done in 3 primary modalities: 
                            <ol>
                                <li>CLI inference (Single and Batched Prompt Inference)
                                </li>
                                <li>Local Inference with Quantized Models
                                </li>
                                <li> <a href="https://github.com/ShishirPatil/gorilla/tree/main/inference#4-private-inference-using-gorilla-hosted-endpoint-on-replicate">Inference with Hosted Endpoint on Replicate</a>
                                </li>
                            </ol>
                    </p>
                    <h5> <a href="https://github.com/ShishirPatil/gorilla/tree/main/inference#1-inference-using-cli"> CLI Inference</a>: Install dependencies, download model, prompt! </h5>
                    <p>
                            <br><code>conda create -n gorilla python=3.10</code>
                            <br><code>conda activate gorilla</code>
                            <br><code>pip install -r requirements.txt</code>
                            <br>
                            <br>At this point you have multiple options as to which Gorilla Model you would like to use. 
                            In order to comply with the Llama license, we have released the Gorilla delta weights, which need to be merged with Llama. 
                            If you would like to use models out of the box, you can directly download <a href="https://huggingface.co/gorilla-llm/gorilla-mpt-7b-hf-v0">gorilla-mpt-7b-hf-v0</a> and <a href="https://huggingface.co/gorilla-llm/gorilla-falcon-7b-hf-v0">gorilla-falcon-7b-hf-v0</a> from Hugging Face. 
                            Run the model with:
                            <br><code>python3 serve/gorilla_falcon_cli.py --model-path path/to/gorilla-falcon-7b-hf-v0</code>
                            <br> To run inference with <a href="https://github.com/ShishirPatil/gorilla/tree/main/inference#2-batch-inference-on-a-prompt-file">batched prompts</a> (multiple prompts in one batch), create a json file following <a href="https://github.com/ShishirPatil/gorilla/blob/main/inference/example_questions/example_questions.jsonl">this format</a> with all of your prompts.
                            Get results for your set of prompts by running:
                            <br><code>python3 gorilla_eval.py --model-path path/to/gorilla-falcon-7b-hf-v0 --question-file path/to/questions.jsonl ----answer-file path/to/answers.jsonl</code>
                            

                    </p>
                    <h5> <a href="https://github.com/ShishirPatil/gorilla/tree/main/inference#3-local-inference-of-quantized-models"> Local Inference With Quantized Models</a></h5>
                    <p>
                        To support running Gorilla locally, we have released quantized versions of the llama, falcon, and mpt-based versions of Gorilla. 
                        The easiest way to run Gorilla locally with a clean UI is through <a href="https://github.com/oobabooga/text-generation-webui">text-generation-webui</a>. 
                        <ol>
                            <li>Clone text-generation-webui:
                                <br><code>git clone https://github.com/oobabooga/text-generation-webui.git</code>
                            </li>
                            <li>Go to cloned folder and install dependencies:
                                <br><code>cd text-generation-webui</code>
                                <br><code>pip install -r requirements.txt</code>
                            </li>
                            <li>
                                Run the following command to start the UI:
                                <br><code>./start_macos.sh </code>
                            </li>
                            <li>
                                Open a browser and paste the following url:
                                <br><code>http://127.0.0.1:7860/</code>
                            </li>
                        </ol>
                        <br>From here, navigate to the ‚ÄúModel‚Äù tab, select your desired quantized model by pasting the name from <a href="https://huggingface.co/gorilla-llm">this Hugging Face repo</a>. 
                        For example in the first text field under ‚ÄúModel,‚Äù one could paste ‚Äúgorilla-llm/gorilla-7b-hf-v1-gguf‚Äù and then specify the specific model as ‚Äúgorilla-7b-hf-v1-q3_K_M.gguf.‚Äù 
                        After loading the model, navigate to the ‚ÄúChat‚Äù tab and start prompting!
                        <br>If you want to see an in depth walk through of how quantization with llama.cpp is performed under the hood, check out <a href="https://colab.research.google.com/drive/1JP_MN-J1rODo9k_-dR_9c9EnZRCfcVNe?usp=sharing">this Google Colab</a>.  
                    </p>
                    <h5> <a href ="https://github.com/ShishirPatil/gorilla/tree/main/inference#4-private-inference-using-gorilla-hosted-endpoint-on-replicate">Inference with Hosted Endpoint on Replicate</a></h5>

                    <h4 id="gorilla-openfunctions">Gorilla OpenFunctions: Executable API Calls ‚öîÔ∏è</h4>
                    <i>
                        Note: If you would like to try out Gorilla Open Functions in a playground-like environment check out <a href="https://colab.research.google.com/drive/16M5J2H9F8YQora_W2PDnp120slZH-Mqd">this Google Colab</a>. 
                        <br>For more information about the technical details behind Gorilla Open Functions check out <a href="https://gorilla.cs.berkeley.edu/blogs/4_open_functions.html">this blog post</a>!
                    </i>
                    <p>
                        Gorilla Open Functions returns executable API calls given a specific user query and API documentation. 
                        It acts as  an open source version of OpenAI‚Äôs function calling except with chat completion. 
                        One key difference is that OpenAI will repeatedly prompt users if arguments are missing in the query but Gorilla Open Functions utilizes chat completion to accurately infer and fill in these missing parameters. 
                    </p>
                    <p>
                        Using Gorilla Open Functions is simple: Install the necessary dependencies, provide a query and a set of functions, and get a response.
                        <ol>
                            <li>Install dependencies:
                                <br><code>pip install openai==0.28.1</code>
                            </li>
                            <li> Provide a query and set of functions (example below):
                                <pre><code>
                                    query = "Call me an Uber ride type \"Plus\" in Berkeley at zipcode 94704 in 10 minutes"
                                    function_documentation = {  
                                        "name": "Uber Carpool",
                                        "api_name": "uber.ride",
                                        "description": "Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters",
                                        "parameters": 
                                            [
                                                {
                                                "name": "loc", 
                                                "description": "location of the starting place of the uber ride"
                                                }, 
                                                {
                                                    "name":"type", 
                                                    "enum": ["plus", "comfort", "black"], 
                                                    "description": "types of uber ride user is ordering"
                                                },
                                                {
                                                    "name": "time", 
                                                    "description": "the amount of time in minutes the customer is willing to wait"
                                                }
                                            ]
                                        }
                                </code></pre>
                            </li> 
                            <li> Define a response function (example below):
                                <pre><code>
                                    def get_gorilla_response(prompt="Call me an Uber ride type \"Plus\" in Berkeley at zipcode 94704 in 10 minutes", model="gorilla-openfunctions-v0", functions=[]):
                                        openai.api_key = "EMPTY"
                                        openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"
                                        try:
                                            completion = openai.ChatCompletion.create(
                                            model="gorilla-openfunctions-v1",
                                            temperature=0.0,
                                            messages=[{"role": "user", "content": prompt}],
                                            functions=functions,
                                            )
                                            return completion.choices[0].message.content
                                        except Exception as e:
                                            print(e, model, prompt)
                                </code></pre>
                            </li>
                            <li> Return relevant API call:
                                <br><code>get_gorilla_response(query, functions=functions)</code>
                            </li>
                            <li> Output Example:
                                <br><code>uber.ride(loc="berkeley", type="plus", time=10)</code>
                            </li>
                        </ol>
                    </p>

                    <p>
                        Also, checkout the following tutorials for running and self hosting OpenFunctions locally!
                        <ul>
                            <li> <a href="https://colab.research.google.com/drive/1I9UJoKh9sngE2MfPfQD5kbn2-twq2xvY?usp=sharing">Run OpenFunctions Locally!</a>
                            </li> 
                            <li> <a href="https://github.com/ShishirPatil/gorilla/tree/main/openfunctions#self-hosting-openfunctions">Self-Hosting OpenFunctions</a>
                            </li>
                        </ul>

                    </p>

                    <h4 id="integrate-third-party">Integrating Gorilla with Third Party Libraries ü¶ú</h4>
                    <i> For a self contained walkthrough on how to integrate Gorilla with Langchain check out <a href="https://colab.research.google.com/drive/11HJWR3ylG1HSE2v78W1gRK-dKkSA0pHe">this Google Colab</a>. </i>
                    <p>
                        Integrate Gorilla with Langchain for easy deployment in less than 10 lines of code. 
                        Install dependencies, create a langchain agent and start prompting!
                        <ol>
                            <li> Install dependencies:
                                <br><code>pip install transformers[sentencepiece] datasets langchain openai==0.28.1 &> /dev/null</code>
                            </li>
                            <li>Define Langchain Chat Agent:
                                <pre><code>
                                    from langchain.chat_models import ChatOpenAI
                                    chat_model = ChatOpenAI(
                                        openai_api_base="http://zanino.millennium.berkeley.edu:8000/v1",
                                        openai_api_key="EMPTY",
                                        model="gorilla-7b-hf-v1",
                                        verbose=True
                                    )
                                </code></pre>
                            </li> 
                            <li> Prompt:
                                <br><code>example = chat_model.predict("I want to translate from English to Chinese")</code>
                                <br><code>print(example)</code>
                            </li>
                        </ol>
                    </p>

                    <h4 id="api-zoo">Gorilla API Zoo Index üìñ</h4>
                    <p>
                        However, the promise of retriever-aware training is not fully utilized yet in reality. 
                        This is mainly because the accuracy of the retriever is not good enough. In other words, 
                        the recall for the retriever has become a bottleneck for the final performance of the LLM. 
                        Imagine the model would easily get confused if it got the question to 
                        ‚Äúlook up whether in Berkeley‚Äù but with the supporting documents of ‚Äúbiography of 
                        Albert Einstein‚Äù. Thus, balancing between the recall of the retriever and the frequency 
                        of updating LLMs is a choice to make.
                    </p>

                    <p>
                        Wrapping Up

                        As with all innovations, retriever-aware training comes with its set of pros and cons. 
                        But its introduction marks an exciting shift towards creating LLMs that are more adaptable, 
                        accurate, and less prone to errors. As we continue to refine this methodology, there's 
                        no doubt that the future of LLM training is brimming with potential.

                    </p>

                    <h4 id="train-your-own-gorilla">Train Your Own Gorilla üî®üîß</h4>
                    <p>
                        However, the promise of retriever-aware training is not fully utilized yet in reality. 
                        This is mainly because the accuracy of the retriever is not good enough. In other words, 
                        the recall for the retriever has become a bottleneck for the final performance of the LLM. 
                        Imagine the model would easily get confused if it got the question to 
                        ‚Äúlook up whether in Berkeley‚Äù but with the supporting documents of ‚Äúbiography of 
                        Albert Einstein‚Äù. Thus, balancing between the recall of the retriever and the frequency 
                        of updating LLMs is a choice to make.
                    </p>

                    <p>
                        Wrapping Up

                        As with all innovations, retriever-aware training comes with its set of pros and cons. 
                        But its introduction marks an exciting shift towards creating LLMs that are more adaptable, 
                        accurate, and less prone to errors. As we continue to refine this methodology, there's 
                        no doubt that the future of LLM training is brimming with potential.

                    </p>

                </div>

                <!-- APIs change frequently -->
                <p style="text-align: center; margin-bottom: 0">
                    <img src="../assets/img/blog_post_1_apichange.png" alt="APIs change frequently" width="95%">
                    <br>
                </p>
                <p width="80%" style="text-align:center; margin-left:10%; margin-right:10%; padding-bottom: -10px">
                    <i style="font-size: 0.9em;">
                        APIs evolve frequently! For example, there were 31 API modifications for AWS APIs just yesterday.
                    </i>
                </p>


                <h4 id="evolving-api"> Keeping up with frequently evolving APIs</h4>
                <div class="body">
                    <p>
                        APIs are known to evolve frequently - more freuquently than it is possible to re-train LLMs. So, how can LLMs keep up with this, and not serve the user out-lawed APIs? To handle this, Gorilla, can be used for inference in two modes: <em>zero-shot</em> and <em>with retrieval</em>. In zero-shot, during inference, user provides the prompt in natural language. This can be for a simple task (e.g, "I would like to identify the objects in an image"), or you can specify a vague goal, (e.g, "I am going to the zoo, and would like to track animals"). This prompt (with NO further prompt tuning) is fed to the Gorilla LLM model which then returns the API call that will help in accomplishing the task and/or goal. 
                        In retrieval mode, the retriever first retrieves the most up-to-date API documentation stored in APIZoo, an API Database for LLMs.
                        Before being sent to Gorilla, the API documentation is concatenated to the user prompt along with the message "Use this API documentation for reference:"  The output of Gorilla is an API to be invoked. The retriever aware inference mode, enables Gorilla to be robust to frequent changes in APIs! We have open-sourced our APIZoo, and welcome contributions from the community!   
                    </p>

                </div>

                <!-- Retrievers and LLMs  -->
                <p style="text-align: center; margin-bottom: 0">
                    <img src="../assets/img/blog_post_1_inference.jpg" alt="Gorilla can be used in zero-shot and with retrievers" width="95%">
                    <br>
                </p>
                <p width="80%" style="text-align:center; margin-left:10%; margin-right:10%; padding-bottom: -10px">
                    <i style="font-size: 0.9em;">
                        Gorilla, can be used for inference in two modes: <em>zero-shot</em> and <em>with retrieval</em>. In zero-shot, the prompt is directly fed to the Gorilla LLM model. 
                        In retrieval mode, the retriever first retrieves the most up-to-date API documentation stored in APIZoo. 
                    </i>
                </p>

                <h4 id="llm-and-retrievers">Love at First Query: The Untold Bond of LLMs and Retrievers</h4>
                <div class="body">
                    <p>
                        If you are deploying LLMs in production today, you might be
                        augmenting your model with retrievers such as in Retriever Augmented Generation (RAG) paradigms. Given, most LLMs today are used with retrievers, shouldn't the training recipie for the LLM consider this!! In Gorilla, we consider retrievers to be first class citizens, and train our models to be <em>retreiver aware</em>. If you are thinking about integrating LLMs with llamaindex, vector databases such as Weviate, etc check out our blog post on <a class="continue-link" href="blogs/blog_post_2_rat.html"> Retrieval Aware Training (RAT)</a> where we teach LLMs to "work-together" with retrievers! 
                    </p>
                </div>

                <!-- Gorilla side by side comparison -->
                <p style="text-align: center; margin-bottom: 0">
                    <img src="../assets/img/blog_post_1_result.png" alt="How well does Gorilla perform" width="65%">
                    <br>
                </p>
                <p width="80%" style="text-align:center; margin-left:10%; margin-right:10%; padding-bottom: -10px">
                    <i style="font-size: 0.9em;">
                        Examples of API calls. In this example, for the given prompt GPT-4 presents a model that doesn't exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.
                    </i>
                </p>

                <h4 id="hallucinations">Reality Bytes: When LLMs See Things That Aren't There</h4>
                <div class="body">
                    <p>
                        Hallucination is the center of discussions for all things LLMs. In the context of API generation, hallucination can be defined as the model generating API calls that do not exist. An LLM generation can be in-accurate or it could be hallucinated. One does not mean the other. For example, if the user asks for a classifier for medical images, if the model generates a Stripe API call for a image classifier - it is hallucination, since it doesn't exist! On the other hand, if the model recommends to use the Stripe API for checking your balance, it is an incorrect usage of the API, but atleast not made up (noh-hallucinated). In our blog <a href=""></a> we describe Gorilla's innovative approach of using Abstract Syntax Trees (ASTs) to measure hallucination of the generated API calls. 
                        Though not generalizable to all tasks, to the best of our knowledge, Gorilla is the first to measure and quantify hallucination for LLM generations! 

                    </p>
                </div>
            </div>
        </div>
    </div>

    <style>
        body {
            font-family: 'Source Sans Pro', sans-serif;
            margin: 0;
            padding: 0;
            background: white;
            justify-content: center;
            align-items: center;
        }
        .blog-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }
        .blog-post {
            margin: 20px;
            padding: 20px;
            max-width: 1000px; 
            justify-content: center;
        }
        .blog-post img {
            display: block;
            margin: 0 auto;
        }
        .blog-title{
            color: #055ada;
            text-align: center;
        }

        .author-date {
                display: flex;
                margin-bottom: 0px;
                justify-content: center; 
        }
        .author {
                font-size: 16px;
                color: #1E90FF;
                margin-right: 20px;
        }

        .date {
            font-size: 16px;
            color: #7e8790;
        }

        .preview {
            text-align: justify; 
            text-justify: inter-word; 
        }

        .highlight-clean-blog {
            color: #313437;
            background-color: #fff;
            padding: 50px 0;
        }

        .box-index {
        position: fixed;
        top: 50%; 
        left: 0px; 
        transform: translateY(-50%);
        background-color: #f9f9f9;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        max-width: 150px;
        }

        .box-index h3 {
        font-size: 1.2em;
        margin-bottom: 10px;
        }

        .box-index ul {
        list-style-type: disc;
        padding: 0;
        }

        .box-index ul li {
        margin-bottom: 10px;
        }

        .box-index ul li a {
        text-decoration: none;
        color: #333;
        }

        .box-index ul li a:hover {
        color: #1E90FF;
        }

        
        .more-blogs .sub-menu {
            display: none;
        }

        .more-blogs .sub-menu.expanded {
            display: block;
            max-height: 200px; /* Adjust the max height as needed */
            overflow-y: auto;
        }

        .more-blogs .sub-menu li {
            padding: 10px;
            border-bottom: 1px solid #ccc;
        }

        .more-blogs .sub-menu li:last-child {
            border-bottom: none;
        }

        .more-blogs .caret {
            transition: transform 0.3s ease-in-out;
            display: inline-block;
            transform: rotate(0deg);
            font-size: 12px; /* Adjust the font size to change the caret size */
        }

        .more-blogs.expanded .caret {
            transform: rotate(90deg);
        }

        @media screen and (max-width: 768px) {
        .blog-post {
            padding: 10px; /* Adjust spacing for smaller screens */
        }
        .blog-post img {
            max-width: 80%; /* Reduce image size for smaller screens */
        }
        .box-index {
        display: none; /* Hide the index on smaller screens */
        }
    }

    </style>
</body>
</html>

<script>
    function toggleMoreBlogs() {
        var subMenu = document.querySelector('.more-blogs .sub-menu');
        var parentItem = document.querySelector('.more-blogs');
        subMenu.classList.toggle('expanded');
        parentItem.classList.toggle('expanded');
    }
</script>