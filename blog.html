<!DOCTYPE html>
<html>
<head>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NRZJLJCSH6"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-NRZJLJCSH6');
    </script>


    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Gorilla</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro">
    <link rel="stylesheet" href="assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="assets/css/styles.css">
    <link rel="stylesheet" href="assets/css/Team-Clean.css">
</head>

<body>
       <!-- Navigation Bar -->
    <div class="navbar" style="position: absolute; top: 0; right: 20px; padding: 10px; z-index: 100;">
        <a href="index.html">Home </a>&nbsp;&nbsp;&nbsp;     
        <a href=""> Blog</a>
    </div>
    <div class="highlight-clean-blog" style="padding-bottom: 10px;">
        <h1 class="text-center" style="padding-bottom: 10px;"> ü¶ç Gorilla: Large Language Model Connected with Massive APIs</h1>
        <div class="blog-container">
            <div class="blog-post">
                <h2 class="blog-title">Blog Post Title 1</h2>
                <div class="author-date">
                    <p class="author">Author Name 1  </p>
                    <p class="date">Date 1</p>
                </div>      
                <img src="assets/img/blog_post_2_image.jpg" alt="Blog Post 1 Image">
                <div class="preview">
                    <p>Diffusion models have recently emerged as the de facto standard for generating complex, high-dimensional outputs. You may know them for their ability to produce stunning AI art and hyper-realistic synthetic images, but they have also found success in other applications such as drug design and continuous control. The key idea behind diffusion models is to iteratively transform random noise into a sample, such as an image or protein structure. This is typically motivated as a maximum likelihood estimation problem, where the model is trained to generate samples that match the training data as closely as possible.</p>

                    <p>However, most use cases of diffusion models are not directly concerned with matching the training data, but instead with a downstream objective. We don‚Äôt just want an image that looks like existing images, but one that has a specific type of appearance; we don‚Äôt just want a drug molecule that is physically plausible, but one that is as effective as possible. In this post, we show how diffusion models can be trained on these downstream objectives directly using reinforcement learning (RL). To do this, we finetune Stable Diffusion on a variety of objectives, including image compressibility, human-perceived aesthetic quality, and prompt-image alignment. The last of these objectives uses feedback from a large vision-language model to improve the model‚Äôs performance on unusual prompts, demonstrating how powerful AI models can be used to improve each other without any humans in the loop.</p>
                </div>
                <a class="continue-link" href="blogs/blog_post_1.html">Continue</a>
                <hr class="post-separator">
            </div>
        
            <!-- Blog Post 2 -->
            <div class="blog-post">
                <h2 class="blog-title">Blog Post Title 2</h2>
                <div class="author-date">
                    <p class="author">Author Name 1  </p>
                    <p class="date">Date 1</p>
                </div>
                <img src="assets/img/blog_post_2_image.jpg" alt="Blog Post 2 Image">
                <div class="preview">
                    <p>Diffusion models have recently emerged as the de facto standard for generating complex, high-dimensional outputs. You may know them for their ability to produce stunning AI art and hyper-realistic synthetic images, but they have also found success in other applications such as drug design and continuous control. The key idea behind diffusion models is to iteratively transform random noise into a sample, such as an image or protein structure. This is typically motivated as a maximum likelihood estimation problem, where the model is trained to generate samples that match the training data as closely as possible.</p>

                    <p>However, most use cases of diffusion models are not directly concerned with matching the training data, but instead with a downstream objective. We don‚Äôt just want an image that looks like existing images, but one that has a specific type of appearance; we don‚Äôt just want a drug molecule that is physically plausible, but one that is as effective as possible. In this post, we show how diffusion models can be trained on these downstream objectives directly using reinforcement learning (RL). To do this, we finetune Stable Diffusion on a variety of objectives, including image compressibility, human-perceived aesthetic quality, and prompt-image alignment. The last of these objectives uses feedback from a large vision-language model to improve the model‚Äôs performance on unusual prompts, demonstrating how powerful AI models can be used to improve each other without any humans in the loop.</p>
                </div>
                <a class="continue-link" href="blogs/blog_post_2.html">Continue</a>
                <hr class="post-separator">
            </div>
            </div>
    </div>

<style>
    body {
        font-family: 'Source Sans Pro', sans-serif;
        margin: 0;
        padding: 0;
        background: white;
        justify-content: center;
        align-items: center;
    }
    .blog-container {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
    }
    .blog-post {
        margin: 20px;
        padding: 20px;
        max-width: 1000px; 
        justify-content: center;
    }
    .blog-post img {
        display: block;
        margin: 0 auto;
    }
    .blog-title{
        color: #055ada;
        text-align: center;
    }

    .author-date {
            display: flex;
            margin-bottom: 0px;
            justify-content: center; 
    }
    .author {
            font-size: 16px;
            color: #1E90FF;
            margin-right: 20px;
    }

    .date {
        font-size: 16px;
        color: #7e8790;
    }

    .continue-link {
        display: block;
        margin-top: 10px; 
        text-align: left;
    }

    .highlight-clean-blog {
        color: #313437;
        background-color: #fff;
        padding: 50px 0;
    }

    .post-separator {
        border: 0.5px solid #e8e8e8; 
        margin: 20px 0; 
    }


    .preview {
        text-align: justify; 
        text-justify: inter-word; 
    }

    @media screen and (max-width: 768px) {
    .blog-post {
        padding: 10px; /* Adjust spacing for smaller screens */
    }
    .blog-post img {
        max-width: 80%; /* Reduce image size for smaller screens */
    }


}
</style>
</body>

</html>