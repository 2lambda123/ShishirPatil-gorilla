INPUT_PRICE_PER_MILLION_TOKEN = {
    "claude-3-opus-20240229-FC": 15,
    "claude-3-opus-20240229": 15,
    "claude-3-sonnet-20240229-FC": 3,
    "claude-3-sonnet-20240229": 3,
    "claude-3-haiku-20240307-FC": 0.25,
    "claude-3-haiku-20240307": 0.25,
    "claude-3-5-sonnet-20240620-FC": 3,
    "claude-3-5-sonnet-20240620": 3,
    "claude-2.1": 8,
    "claude-instant-1.2": 0.8,
    "mistral-large-2402-FC-Any": 4,
    "mistral-large-2402-FC-Auto": 4,
    "mistral-medium-2312": 2.7,
    "mistral-small-2402-FC-Any": 1,
    "mistral-small-2402-FC-Auto": 1,
    "mistral-small-2402": 1,
    "mistral-tiny-2312": 0.25,
    "gpt-4o-2024-05-13-FC": 5,
    "gpt-4o-2024-05-13": 5,
    "gpt-4-1106-preview-FC": 10,
    "gpt-4-1106-preview": 10,
    "gpt-4-0125-preview": 10,
    "gpt-4-0125-preview-FC": 10,
    "gpt-4-turbo-2024-04-09-FC": 10,
    "gpt-4-turbo-2024-04-09": 10,
    "gpt-4-0613": 30,
    "gpt-4-0613-FC": 30,
    "gpt-3.5-turbo-0125": 0.5,
    "gpt-3.5-turbo-0125-FC": 0.5,
    "gemini-1.0-pro": 0.5,
    "gemini-1.5-pro-preview-0409": 3.5,
    "gemini-1.5-pro-preview-0514": 3.5,
    "gemini-1.5-flash-preview-0514": 0.35,
    "databricks-dbrx-instruct": 2.25,
    "command-r-plus-FC": 3,
    "command-r-plus": 3,
    "command-r-plus-FC-optimized": 3,
    "command-r-plus-optimized": 3,
}

OUTPUT_PRICE_PER_MILLION_TOKEN = {
    "claude-3-opus-20240229-FC": 75,
    "claude-3-opus-20240229": 75,
    "claude-3-sonnet-20240229-FC": 15,
    "claude-3-sonnet-20240229": 15,
    "claude-3-5-sonnet-20240620-FC": 15,
    "claude-3-5-sonnet-20240620": 15,
    "claude-3-haiku-20240307-FC": 1.25,
    "claude-3-haiku-20240307": 1.25,
    "claude-2.1": 24,
    "claude-instant-1.2": 2.4,
    "mistral-large-2402-FC-Any": 12,
    "mistral-large-2402-FC-Auto": 12,
    "mistral-small-2402": 3,
    "mistral-medium-2312": 8.1,
    "mistral-small-2402-FC-Any": 3,
    "mistral-small-2402-FC-Auto": 3,
    "mistral-tiny-2312": 0.25,
    "gpt-4o-2024-05-13-FC": 15,
    "gpt-4o-2024-05-13": 15,
    "gpt-4-turbo-2024-04-09-FC": 30,
    "gpt-4-turbo-2024-04-09": 30,
    "gpt-4-1106-preview": 30,
    "gpt-4-1106-preview-FC": 30,
    "gpt-4-0125-preview-FC": 30,
    "gpt-4-0125-preview": 30,
    "gpt-4-0613": 60,
    "gpt-4-0613-FC": 60,
    "gpt-3.5-turbo-0125": 1.5,
    "gpt-3.5-turbo-0125-FC": 1.5,
    "gemini-1.0-pro": 1.5,
    "gemini-1.5-pro-preview-0409": 10.50,
    "gemini-1.5-pro-preview-0514": 10.50,
    "gemini-1.5-flash-preview-0514": 0.53,
    "databricks-dbrx-instruct": 6.75,
    "command-r-plus-FC": 15,
    "command-r-plus": 15,
    "command-r-plus-FC-optimized": 15,
    "command-r-plus-optimized": 15,
}

# The latency of the open-source models are hardcoded here.
# Because we do batching when generating the data, so the latency is not 
# accurate from the result data.
# This is the latency for the whole batch of data, when using 8 V100 GPUs.
OSS_LATENCY = {
    "deepseek-ai/deepseek-coder-6.7b-instruct": 909,
    "google/gemma-7b-it": 95,
    "NousResearch/Hermes-2-Pro-Mistral-7B": 135,
    "meta-llama/Meta-Llama-3-8B-Instruct": 73,
    "meta-llama/Meta-Llama-3-70B-Instruct": 307,
    "gorilla-openfunctions-v2": 83,
    "THUDM/glm-4-9b-chat": 223
}

# Price got from Azure, 22.032 per hour for 8 V100, Pay As You Go Total Price
# Reference: https://azure.microsoft.com/en-us/pricing/details/machine-learning/
V100_x8_PRICE_PER_HOUR = 22.032

NO_COST_MODELS = [
    "Nexusflow-Raven-v2",
    "firefunction-v1-FC",
    "firefunction-v2-FC",
    "meetkai/functionary-medium-v2.4-FC",
    "meetkai/functionary-small-v2.2-FC",
    "meetkai/functionary-small-v2.4-FC",
    "snowflake/arctic",
    "nvidia/nemotron-4-340b-instruct",
    "THUDM/glm-4-9b-chat",
]