{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.parse_results import get_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-2024-05-13',\n",
       " 'pt-research-llama-3-70b-instruct-generic-oai-compatible-model',\n",
       " 'with-bfcl-parsing-temp0_7-pt-research-dbrx-instruct',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-4-0125-preview-FC',\n",
       " 'with-bfcl-parsing-pt-research-dbrx-instruct',\n",
       " 'agent-flan-llama3-8b-lr2e-7-2ep-generic-oai-compatible-model',\n",
       " 'gpt-3.5-turbo-0125-FC',\n",
       " 'gpt-4-0125-preview',\n",
       " 'databricks-dbrx-instruct-generic-oai-compatible-model-FC',\n",
       " 'gpt-4-turbo-2024-04-09',\n",
       " 'pt-research-llama-3-8b-instruct-generic-oai-compatible-model',\n",
       " 'noparse-pt-research-dbrx-instruct-generic-oai-compatible-model']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_PATH = \"../result/\"\n",
    "OUTPUT_PATH = \"../score/\"\n",
    "\n",
    "entries = os.scandir(OUTPUT_PATH)\n",
    "subdirs = [entry.path for entry in entries if entry.is_dir()]\n",
    "acc_df_list = []\n",
    "model_names = [subdir.split(OUTPUT_PATH)[1] for subdir in subdirs]\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'databricks-dbrx-instruct-generic-oai-compatible-model-FC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_df(models=['databricks-dbrx-instruct-old-run']):\n",
    "    error_results_df_list = []\n",
    "    full_results_df_list = []\n",
    "    acc_dict = {'model': [], 'filename': [], 'accuracy': [], 'correct_count': [], 'total_count': []}\n",
    "    for model in models:\n",
    "        # model = 'gpt-3.5-turbo-0125'\n",
    "        results_dir = f'../score/{model}'\n",
    "        json_files = [f'{results_dir}/{f}' for f in os.listdir(results_dir) if f.endswith('.json')]\n",
    "        for filename in json_files:\n",
    "            with open(filename, 'r') as f:\n",
    "                try:\n",
    "                    data = [json.loads(line) for line in f.readlines()]\n",
    "                    # skip the accuracy line\n",
    "                    df = pd.DataFrame(data[1:])\n",
    "                    df['filename'] = filename.split('/')[-1]\n",
    "                    error_results_df_list.append(df)\n",
    "                    # parse out accuracy_info\n",
    "                    acc_info = data[0]\n",
    "                    acc_dict['filename'].append(filename.split('/')[-1])\n",
    "                    acc_info['model'] = model\n",
    "                    for key in acc_info.keys():\n",
    "                        acc_dict[key].append(acc_info[key])\n",
    "                except Exception as e:\n",
    "                    print(f'Error reading {filename}: {e}')\n",
    "                    \n",
    "    # now read full results\n",
    "    for model in models:\n",
    "        results_dir = f'../result/{model}'\n",
    "        json_files = [f'{results_dir}/{f}' for f in os.listdir(results_dir) if f.endswith('.json')]\n",
    "        for filename in json_files:\n",
    "            with open(filename, 'r') as f:\n",
    "                try:\n",
    "                    data = [json.loads(line) for line in f.readlines()]\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df['filename'] = filename.split('/')[-1]\n",
    "                    df['model_name'] = model\n",
    "                    full_results_df_list.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f'Error reading {filename}: {e}')\n",
    "\n",
    "    acc_df = pd.DataFrame(acc_dict)\n",
    "    acc_df['metric'] = acc_df['filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    error_result_df = pd.concat(error_results_df_list)\n",
    "    full_result_df = pd.concat(full_results_df_list)\n",
    "\n",
    "    for model in acc_df['model'].unique():\n",
    "        acc = acc_df[acc_df['model'] == model].correct_count.sum() / acc_df[acc_df['model'] == model].total_count.sum()\n",
    "        print(f'Model: {model} : Acc = {100.0*acc}%')\n",
    "\n",
    "    return acc_df, error_result_df, full_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: databricks-dbrx-instruct-generic-oai-compatible-model-FC : Acc = 51.61290322580645%\n",
      "Model: gpt-4-0125-preview-FC : Acc = 87.25806451612902%\n"
     ]
    }
   ],
   "source": [
    "# compare DBRX and GPT-4 to see how we can get up there\n",
    "\n",
    "acc_df, error_result_df, full_result_df = get_results_df(models=[\n",
    "            'databricks-dbrx-instruct-generic-oai-compatible-model-FC',\n",
    "            'gpt-4-0125-preview-FC',\n",
    "                                                                 ])\n",
    "\n",
    "full_result_df['test_category'] = full_result_df['filename'].apply(lambda x: x.split(\"gorilla_openfunctions_v1_test_\")[1].split(\"_result.json\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_category</th>\n",
       "      <th>valid</th>\n",
       "      <th>error</th>\n",
       "      <th>error_type</th>\n",
       "      <th>model_result</th>\n",
       "      <th>decoded_result</th>\n",
       "      <th>filename</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_result_raw</th>\n",
       "      <th>model_result_decoded</th>\n",
       "      <th>possible_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>generic-oai-compatible-model-FC</td>\n",
       "      <td>relevance</td>\n",
       "      <td>False</td>\n",
       "      <td>[Valid syntax. Successfully decode AST when it...</td>\n",
       "      <td>relevance_error:decoder_success</td>\n",
       "      <td>[{'find_roots': '{\"a\": 0, \"b\": 1, \"c\": 2}'}]</td>\n",
       "      <td>[{'find_roots': {'a': 0, 'b': 1, 'c': 2}}]</td>\n",
       "      <td>relevance_score.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>generic-oai-compatible-model-FC</td>\n",
       "      <td>relevance</td>\n",
       "      <td>False</td>\n",
       "      <td>[Valid syntax. Successfully decode AST when it...</td>\n",
       "      <td>relevance_error:decoder_success</td>\n",
       "      <td>[{'math_integral_calculator': '{\"function\": \"3...</td>\n",
       "      <td>[{'math_integral_calculator': {'function': '3*...</td>\n",
       "      <td>relevance_score.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>generic-oai-compatible-model-FC</td>\n",
       "      <td>relevance</td>\n",
       "      <td>False</td>\n",
       "      <td>[Valid syntax. Successfully decode AST when it...</td>\n",
       "      <td>relevance_error:decoder_success</td>\n",
       "      <td>[{'get_closest_prime': '{\"number\": 30, \"skip\":...</td>\n",
       "      <td>[{'get_closest_prime': {'number': 30, 'skip': ...</td>\n",
       "      <td>relevance_score.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>generic-oai-compatible-model-FC</td>\n",
       "      <td>relevance</td>\n",
       "      <td>False</td>\n",
       "      <td>[Valid syntax. Successfully decode AST when it...</td>\n",
       "      <td>relevance_error:decoder_success</td>\n",
       "      <td>[{'calculate_maximum_height': '{\"gravity\": 9.8...</td>\n",
       "      <td>[{'calculate_maximum_height': {'gravity': 9.8,...</td>\n",
       "      <td>relevance_score.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>generic-oai-compatible-model-FC</td>\n",
       "      <td>relevance</td>\n",
       "      <td>False</td>\n",
       "      <td>[Valid syntax. Successfully decode AST when it...</td>\n",
       "      <td>relevance_error:decoder_success</td>\n",
       "      <td>[{'calculate_projectile_range': '{\"angle\": 45,...</td>\n",
       "      <td>[{'calculate_projectile_range': {'angle': 45, ...</td>\n",
       "      <td>relevance_score.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       model_name test_category  valid  \\\n",
       "0   5  generic-oai-compatible-model-FC     relevance  False   \n",
       "1   8  generic-oai-compatible-model-FC     relevance  False   \n",
       "2  12  generic-oai-compatible-model-FC     relevance  False   \n",
       "3  15  generic-oai-compatible-model-FC     relevance  False   \n",
       "4  17  generic-oai-compatible-model-FC     relevance  False   \n",
       "\n",
       "                                               error  \\\n",
       "0  [Valid syntax. Successfully decode AST when it...   \n",
       "1  [Valid syntax. Successfully decode AST when it...   \n",
       "2  [Valid syntax. Successfully decode AST when it...   \n",
       "3  [Valid syntax. Successfully decode AST when it...   \n",
       "4  [Valid syntax. Successfully decode AST when it...   \n",
       "\n",
       "                        error_type  \\\n",
       "0  relevance_error:decoder_success   \n",
       "1  relevance_error:decoder_success   \n",
       "2  relevance_error:decoder_success   \n",
       "3  relevance_error:decoder_success   \n",
       "4  relevance_error:decoder_success   \n",
       "\n",
       "                                        model_result  \\\n",
       "0       [{'find_roots': '{\"a\": 0, \"b\": 1, \"c\": 2}'}]   \n",
       "1  [{'math_integral_calculator': '{\"function\": \"3...   \n",
       "2  [{'get_closest_prime': '{\"number\": 30, \"skip\":...   \n",
       "3  [{'calculate_maximum_height': '{\"gravity\": 9.8...   \n",
       "4  [{'calculate_projectile_range': '{\"angle\": 45,...   \n",
       "\n",
       "                                      decoded_result              filename  \\\n",
       "0         [{'find_roots': {'a': 0, 'b': 1, 'c': 2}}]  relevance_score.json   \n",
       "1  [{'math_integral_calculator': {'function': '3*...  relevance_score.json   \n",
       "2  [{'get_closest_prime': {'number': 30, 'skip': ...  relevance_score.json   \n",
       "3  [{'calculate_maximum_height': {'gravity': 9.8,...  relevance_score.json   \n",
       "4  [{'calculate_projectile_range': {'angle': 45, ...  relevance_score.json   \n",
       "\n",
       "  prompt model_result_raw model_result_decoded possible_answer  \n",
       "0    NaN              NaN                  NaN             NaN  \n",
       "1    NaN              NaN                  NaN             NaN  \n",
       "2    NaN              NaN                  NaN             NaN  \n",
       "3    NaN              NaN                  NaN             NaN  \n",
       "4    NaN              NaN                  NaN             NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['relevance' 'multiple_function' 'parallel_multiple_function'\n",
      " 'parallel_function' 'simple']\n",
      "['parallel_function' 'parallel_multiple_function' 'relevance'\n",
      " 'multiple_function' 'simple']\n"
     ]
    }
   ],
   "source": [
    "print(error_result_df.test_category.unique())\n",
    "print(full_result_df.test_category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_by_id(idx_list, dbrx_errors_df, dbrx_results_df, gpt4_errors_df, gpt4_results_df, category=\"simple\"):\n",
    "    if not isinstance(idx_list, list):\n",
    "        idx_list = [idx_list]\n",
    "    for idx in idx_list:\n",
    "        print(\"----------------------------------------------------------------------------\")\n",
    "        dbrx_error = dbrx_errors_df[dbrx_errors_df['id'] == idx]\n",
    "        gpt4_result = gpt4_results_df[gpt4_results_df['idx'] == idx-1]\n",
    "        dbrx_result = dbrx_results_df[dbrx_results_df['idx'] == idx-1]\n",
    "        # need to filter by category because idx is not unique\n",
    "        dbrx_error = dbrx_error[dbrx_error['test_category'] == category]\n",
    "        dbrx_result = dbrx_result[dbrx_result['test_category'] == category]\n",
    "        gpt4_result = gpt4_result[gpt4_result['test_category'] == category]\n",
    "        print(f\"Idx: {dbrx_error.id.values[0]}\")\n",
    "        print(f\"Filename: {dbrx_error.filename.values[0]}\")\n",
    "        # print(f\"Prompt: {json.dumps(dbrx_error.prompt.item(), indent=2)}\")\n",
    "        print(f\"Question: {json.dumps(dbrx_error.prompt.values[0]['question'], indent=2)}\")\n",
    "        print(f\"Dbrx (error_df) result_raw: {json.dumps(dbrx_error.model_result_raw.values[0][0], indent=2)}\")\n",
    "        print(f\"Dbrx error: {json.dumps(dbrx_error.error.values[0], indent=2)}\")\n",
    "\n",
    "        print(f\"GPT4 result_raw: {json.dumps(gpt4_result.result.values[0][0], indent=2)}\")\n",
    "        print(\"\\n----------------------------------------------------------------------------\\n\")\n",
    "        # print(f\"Dbrx result_raw: {json.dumps(dbrx_result.result.item()[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['databricks-dbrx-instruct-generic-oai-compatible-model-FC',\n",
       "       'gpt-4-0125-preview-FC'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_result_df.model_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_results_df = full_result_df[(full_result_df['model_name'] == 'gpt-4-0125-preview-FC')]\n",
    "gpt4_errors_df = error_result_df[(error_result_df['model_name'] == 'gpt-4-0125-preview-FC')]\n",
    "\n",
    "dbrx_results_df = full_result_df[(full_result_df['model_name'] == 'databricks-dbrx-instruct-generic-oai-compatible-model-FC')]\n",
    "dbrx_errors_df = error_result_df[(error_result_df['model_name'] == 'generic-oai-compatible-model-FC')]\n",
    "\n",
    "# probably wanna do this by category\n",
    "def get_dbrx_only_error_ids(dbrx_errors_df, gpt4_errors_df, category=None):\n",
    "    if category:\n",
    "        df1 = dbrx_errors_df[dbrx_errors_df['test_category'] == category]\n",
    "        df2 = gpt4_errors_df[gpt4_errors_df['test_category'] == category]\n",
    "    else:\n",
    "        df1 = dbrx_errors_df\n",
    "        df2 = gpt4_errors_df\n",
    "    dbrx_only_error_ids = list(set(df1.id.unique()) - set(df2.id.unique()))\n",
    "    return dbrx_only_error_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['relevance', 'multiple_function', 'parallel_multiple_function',\n",
       "       'parallel_function', 'simple'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbrx_errors_df.test_category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbrx_only_simple_error_ids = get_dbrx_only_error_ids(dbrx_errors_df,\n",
    "                                                     gpt4_errors_df,\n",
    "                                                     category='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Idx: 131\n",
      "Filename: simple_score.json\n",
      "Question: \"What's the NPV (Net Present Value) of a series of cash flows: [-50000, 10000, 15000, 20000, 25000, 30000] discounted at 8% annually?\"\n",
      "Dbrx (error_df) result_raw: {\n",
      "  \"finance_calculator_npv\": \"{\\\"cash_flows\\\": [10000, 15000, 20000, 25000, 30000], \\\"discount_rate\\\": 0.08, \\\"years\\\": [1, 2, 3, 4, 5] }\"\n",
      "}\n",
      "Dbrx error: [\n",
      "  \"Invalid value for parameter 'cash_flows': [10000, 15000, 20000, 25000, 30000]. Expected one of [[-50000, 10000, 15000, 20000, 25000, 30000]].\"\n",
      "]\n",
      "GPT4 result_raw: {\n",
      "  \"finance_calculator_npv\": \"{\\\"cash_flows\\\":[-50000,10000,15000,20000,25000,30000],\\\"discount_rate\\\":0.08}\"\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_error_by_id(dbrx_only_simple_error_ids[0],\n",
    "                  dbrx_errors_df,\n",
    "                  dbrx_results_df,\n",
    "                  gpt4_errors_df,\n",
    "                  gpt4_results_df,\n",
    "                  category='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Idx: 260\n",
      "Filename: simple_score.json\n",
      "Question: \"Calculate the total quantity of paint needed to cover a wall of 30 feet by 12 feet using a specific brand that covers 400 square feet per gallon.\"\n",
      "Dbrx (error_df) result_raw: {\n",
      "  \"calculate_paint_needed\": \"{\\\"coverage_rate\\\": 400, \\\"height\\\": 30, \\\"length\\\": 12}\"\n",
      "}\n",
      "Dbrx error: [\n",
      "  \"Invalid value for parameter 'height': 30. Expected one of [12].\"\n",
      "]\n",
      "GPT4 result_raw: {\n",
      "  \"calculate_paint_needed\": \"{\\\"coverage_rate\\\":400,\\\"length\\\":30,\\\"height\\\":12}\"\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_error_by_id(dbrx_only_simple_error_ids[1],\n",
    "                  dbrx_errors_df,\n",
    "                  dbrx_results_df,\n",
    "                  gpt4_errors_df,\n",
    "                  gpt4_results_df,\n",
    "                  category='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Idx: 4\n",
      "Filename: simple_score.json\n",
      "Question: \"Find the roots of a quadratic equation with coefficients a=1, b=-3, c=2.\"\n",
      "Dbrx (error_df) result_raw: {\n",
      "  \"algebra_quadratic_roots\": \"{\\\"a\\\": 1, \\\"b\\\": 3, \\\"c\\\": 2}\"\n",
      "}\n",
      "Dbrx error: [\n",
      "  \"Invalid value for parameter 'b': 3. Expected one of [-3].\"\n",
      "]\n",
      "GPT4 result_raw: {\n",
      "  \"algebra_quadratic_roots\": \"{\\\"a\\\":1,\\\"b\\\":-3,\\\"c\\\":2}\"\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_error_by_id(dbrx_only_simple_error_ids[2],\n",
    "                  dbrx_errors_df,\n",
    "                  dbrx_results_df,\n",
    "                  gpt4_errors_df,\n",
    "                  gpt4_results_df,\n",
    "                  category='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Idx: 133\n",
      "Filename: simple_score.json\n",
      "Question: \"Calculate the company's return on equity given its net income of $2,000,000, shareholder's equity of $10,000,000, and dividends paid of $200,000.\"\n",
      "Dbrx (error_df) result_raw: {\n",
      "  \"calculate_return_on_equity\": \"{\\\"net_income\\\": 2000000, \\\"shareholder_equity\\\": 10000000}\"\n",
      "}\n",
      "Dbrx error: [\n",
      "  \"Optional parameter 'dividends_paid' not provided and not marked as optional.\"\n",
      "]\n",
      "GPT4 result_raw: {\n",
      "  \"calculate_return_on_equity\": \"{\\\"net_income\\\":2000000,\\\"shareholder_equity\\\":10000000,\\\"dividends_paid\\\":200000}\"\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_error_by_id(dbrx_only_simple_error_ids[3],\n",
    "                  dbrx_errors_df,\n",
    "                  dbrx_results_df,\n",
    "                  gpt4_errors_df,\n",
    "                  gpt4_results_df,\n",
    "                  category='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function to rerun the example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BFCL-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
